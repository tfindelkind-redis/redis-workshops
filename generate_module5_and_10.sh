#!/bin/bash

echo "üöÄ Generating Module 5 and 10 notebooks..."
echo

# Module 5 - Cost Optimization
python3 << 'PYTHON5'
import json

def mk_md(source):
    return {"cell_type": "markdown", "metadata": {}, "source": source}

def mk_code(source):
    return {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": source}

cells = []

cells.append(mk_md([
    "# Module 5: Cost Optimization & Operational Excellence\n",
    "\n",
    "## üéØ Interactive Lab: Optimizing Redis Costs\n",
    "\n",
    "**Duration:** 45 minutes  \n",
    "**Level:** Intermediate  \n",
    "\n",
    "In this lab, you'll:\n",
    "- üí∞ Understand Azure Redis pricing models\n",
    "- üìä Monitor memory usage and efficiency\n",
    "- üîç Identify cost optimization opportunities\n",
    "- ‚ö° Implement memory-efficient patterns\n",
    "- üìà Calculate cost savings\n",
    "\n",
    "---\n"
]))

cells.append(mk_md(["## Part 1: Setup\n"]))

cells.append(mk_code([
    "!pip install -q redis\n",
    "\n",
    "import redis\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    r.ping()\n",
    "    print('‚úÖ Connected to Redis')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Connection failed: {e}')\n",
    "    print('   Make sure Redis is running')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 2: Azure Redis Pricing Overview\n",
    "\n",
    "### Pricing Tiers (Monthly Estimates)\n",
    "\n",
    "| Tier | Size | Memory | Est. Cost/Month |\n",
    "|------|------|--------|----------------|\n",
    "| **Basic C1** | 1 GB | 1 GB | ~$18 |\n",
    "| **Basic C2** | 2.5 GB | 2.5 GB | ~$37 |\n",
    "| **Standard C1** | 1 GB | 1 GB | ~$55 |\n",
    "| **Standard C2** | 2.5 GB | 2.5 GB | ~$110 |\n",
    "| **Premium P1** | 6 GB | 6 GB | ~$237 |\n",
    "| **Premium P2** | 13 GB | 13 GB | ~$503 |\n",
    "\n",
    "### Cost Factors\n",
    "\n",
    "1. **Tier Selection** - Basic vs Standard vs Premium\n",
    "2. **Size** - Memory capacity\n",
    "3. **Replication** - Additional replicas\n",
    "4. **Persistence** - RDB/AOF snapshots\n",
    "5. **Data Transfer** - Egress costs\n",
    "\n",
    "### üí° Cost Optimization Strategies\n",
    "\n",
    "1. **Right-size your instance**\n",
    "2. **Use appropriate data structures**\n",
    "3. **Set TTLs on keys**\n",
    "4. **Monitor memory usage**\n",
    "5. **Use compression when beneficial**\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 3: Memory Usage Analysis\n",
    "\n",
    "Let's analyze memory usage patterns:\n"
]))

cells.append(mk_code([
    "def get_memory_stats():\n",
    "    \"\"\"Get current memory statistics\"\"\"\n",
    "    info = r.info('memory')\n",
    "    \n",
    "    used_mb = info['used_memory'] / 1024 / 1024\n",
    "    peak_mb = info['used_memory_peak'] / 1024 / 1024\n",
    "    rss_mb = info['used_memory_rss'] / 1024 / 1024\n",
    "    \n",
    "    return {\n",
    "        'used_memory_mb': round(used_mb, 2),\n",
    "        'peak_memory_mb': round(peak_mb, 2),\n",
    "        'rss_memory_mb': round(rss_mb, 2),\n",
    "        'fragmentation_ratio': info.get('mem_fragmentation_ratio', 1.0)\n",
    "    }\n",
    "\n",
    "# Get initial stats\n",
    "stats = get_memory_stats()\n",
    "print('üìä Current Memory Usage:')\n",
    "print(f'   Used: {stats[\"used_memory_mb\"]} MB')\n",
    "print(f'   Peak: {stats[\"peak_memory_mb\"]} MB')\n",
    "print(f'   RSS: {stats[\"rss_memory_mb\"]} MB')\n",
    "print(f'   Fragmentation: {stats[\"fragmentation_ratio\"]:.2f}')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 4: Data Structure Efficiency\n",
    "\n",
    "Different data structures have different memory footprints:\n"
]))

cells.append(mk_code([
    "import sys\n",
    "\n",
    "def compare_data_structure_memory():\n",
    "    \"\"\"Compare memory usage of different approaches\"\"\"\n",
    "    \n",
    "    # Clear existing data\n",
    "    r.flushdb()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Approach 1: Individual string keys\n",
    "    initial_mem = get_memory_stats()['used_memory_mb']\n",
    "    \n",
    "    for i in range(1000):\n",
    "        r.set(f'user:{i}:name', f'User{i}')\n",
    "        r.set(f'user:{i}:email', f'user{i}@example.com')\n",
    "        r.set(f'user:{i}:age', 25 + (i % 50))\n",
    "    \n",
    "    string_mem = get_memory_stats()['used_memory_mb'] - initial_mem\n",
    "    results.append(('Individual Strings (3000 keys)', string_mem))\n",
    "    \n",
    "    # Clean up\n",
    "    r.flushdb()\n",
    "    \n",
    "    # Approach 2: Hashes\n",
    "    initial_mem = get_memory_stats()['used_memory_mb']\n",
    "    \n",
    "    for i in range(1000):\n",
    "        r.hset(f'user:{i}', mapping={\n",
    "            'name': f'User{i}',\n",
    "            'email': f'user{i}@example.com',\n",
    "            'age': 25 + (i % 50)\n",
    "        })\n",
    "    \n",
    "    hash_mem = get_memory_stats()['used_memory_mb'] - initial_mem\n",
    "    results.append(('Hashes (1000 keys)', hash_mem))\n",
    "    \n",
    "    # Display results\n",
    "    print('üîç Memory Usage Comparison (1000 users, 3 fields each):')\n",
    "    print()\n",
    "    print(f'{\"Approach\":<30} | {\"Memory (MB)\":<12} | {\"Savings\"}')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    baseline = results[0][1]\n",
    "    for approach, mem in results:\n",
    "        savings = ((baseline - mem) / baseline * 100) if mem < baseline else 0\n",
    "        print(f'{approach:<30} | {mem:>11.2f} | {savings:>5.1f}%')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison = compare_data_structure_memory()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 5: TTL Management for Cost Control\n",
    "\n",
    "Setting Time-To-Live (TTL) prevents memory bloat:\n"
]))

cells.append(mk_code([
    "def demonstrate_ttl_impact():\n",
    "    \"\"\"Show impact of TTL on memory management\"\"\"\n",
    "    \n",
    "    # Clear database\n",
    "    r.flushdb()\n",
    "    \n",
    "    print('üìä Creating cache entries...')\n",
    "    \n",
    "    # Scenario 1: No TTL (memory grows forever)\n",
    "    print('\\n‚ùå Without TTL:')\n",
    "    for i in range(100):\n",
    "        r.set(f'cache:no_ttl:{i}', f'data_{i}' * 100)\n",
    "    \n",
    "    no_ttl_keys = len(r.keys('cache:no_ttl:*'))\n",
    "    print(f'   Keys: {no_ttl_keys}')\n",
    "    print(f'   Will stay in memory indefinitely')\n",
    "    \n",
    "    # Scenario 2: With TTL (automatic cleanup)\n",
    "    print('\\n‚úÖ With 60s TTL:')\n",
    "    for i in range(100):\n",
    "        r.setex(f'cache:with_ttl:{i}', 60, f'data_{i}' * 100)\n",
    "    \n",
    "    with_ttl_keys = len(r.keys('cache:with_ttl:*'))\n",
    "    print(f'   Keys: {with_ttl_keys}')\n",
    "    print(f'   Will auto-expire in 60 seconds')\n",
    "    \n",
    "    # Show memory stats\n",
    "    stats = get_memory_stats()\n",
    "    print(f'\\nüíæ Current memory usage: {stats[\"used_memory_mb\"]} MB')\n",
    "    \n",
    "    # Calculate monthly cost impact\n",
    "    print('\\nÔøΩÔøΩ Cost Impact (Example):')\n",
    "    print('   Without TTL: Memory keeps growing ‚Üí Requires larger instance')\n",
    "    print('   With TTL: Memory auto-managed ‚Üí Can use smaller instance')\n",
    "    print('   Potential savings: $50-200/month per GB saved')\n",
    "\n",
    "# Run demonstration\n",
    "demonstrate_ttl_impact()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 6: Right-Sizing Calculator\n",
    "\n",
    "Determine the optimal Redis instance size:\n"
]))

cells.append(mk_code([
    "def estimate_redis_size(num_keys, avg_value_size_kb, overhead_factor=1.5):\n",
    "    \"\"\"\n",
    "    Estimate required Redis memory\n",
    "    \n",
    "    Args:\n",
    "        num_keys: Number of keys\n",
    "        avg_value_size_kb: Average value size in KB\n",
    "        overhead_factor: Multiplier for Redis overhead (default 1.5 = 50% overhead)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate raw data size\n",
    "    raw_size_mb = (num_keys * avg_value_size_kb) / 1024\n",
    "    \n",
    "    # Add Redis overhead\n",
    "    estimated_mb = raw_size_mb * overhead_factor\n",
    "    \n",
    "    # Add 20% buffer for growth\n",
    "    recommended_mb = estimated_mb * 1.2\n",
    "    \n",
    "    # Suggest tier\n",
    "    tiers = [\n",
    "        ('C0 (250 MB)', 250, 15),\n",
    "        ('C1 (1 GB)', 1024, 55),\n",
    "        ('C2 (2.5 GB)', 2560, 110),\n",
    "        ('C3 (6 GB)', 6144, 239),\n",
    "        ('P1 (6 GB)', 6144, 237),\n",
    "        ('P2 (13 GB)', 13312, 503),\n",
    "        ('P3 (26 GB)', 26624, 1058),\n",
    "    ]\n",
    "    \n",
    "    suggested_tier = None\n",
    "    for tier_name, tier_mb, cost in tiers:\n",
    "        if tier_mb >= recommended_mb:\n",
    "            suggested_tier = (tier_name, cost)\n",
    "            break\n",
    "    \n",
    "    print('üßÆ Redis Size Calculator')\n",
    "    print()\n",
    "    print(f'üìä Your Workload:')\n",
    "    print(f'   Keys: {num_keys:,}')\n",
    "    print(f'   Avg value size: {avg_value_size_kb} KB')\n",
    "    print()\n",
    "    print(f'üíæ Memory Estimates:')\n",
    "    print(f'   Raw data: {raw_size_mb:.1f} MB')\n",
    "    print(f'   With overhead: {estimated_mb:.1f} MB')\n",
    "    print(f'   Recommended: {recommended_mb:.1f} MB (includes 20% growth buffer)')\n",
    "    print()\n",
    "    \n",
    "    if suggested_tier:\n",
    "        print(f'‚úÖ Suggested Tier: {suggested_tier[0]}')\n",
    "        print(f'   Estimated cost: ${suggested_tier[1]}/month')\n",
    "    else:\n",
    "        print('‚ùå Workload exceeds largest tier - consider clustering')\n",
    "\n",
    "# Example 1: Small cache\n",
    "print('Example 1: Small API Cache')\n",
    "estimate_redis_size(num_keys=10000, avg_value_size_kb=2)\n",
    "\n",
    "print('\\n' + '='*60 + '\\n')\n",
    "\n",
    "# Example 2: Session store\n",
    "print('Example 2: Session Store')\n",
    "estimate_redis_size(num_keys=100000, avg_value_size_kb=5)\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 7: Memory Optimization Best Practices\n",
    "\n",
    "### ‚úÖ Do's\n",
    "\n",
    "1. **Use Hashes for objects** - More memory efficient than separate keys\n",
    "2. **Set TTLs on temporary data** - Prevent unbounded growth\n",
    "3. **Monitor memory usage** - Set up alerts at 80% capacity\n",
    "4. **Use appropriate data structures** - Sorted Sets for rankings, Hashes for objects\n",
    "5. **Compress large values** - For values > 100KB\n",
    "\n",
    "### ‚ùå Don'ts\n",
    "\n",
    "1. **Don't store very large values** - Keep values < 1MB\n",
    "2. **Don't use Redis as primary database** - Use as cache/session store\n",
    "3. **Don't skip maxmemory policy** - Set eviction policy\n",
    "4. **Don't ignore fragmentation** - Monitor and address\n",
    "5. **Don't over-provision** - Start small, scale up as needed\n"
]))

cells.append(mk_md(["## Cleanup\n"]))

cells.append(mk_code([
    "# Clean up test data\n",
    "r.flushdb()\n",
    "print('‚úÖ Cleanup complete')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### üí∞ Cost Optimization\n",
    "\n",
    "1. **Right-Size Your Instance**\n",
    "   - Start with smaller tier\n",
    "   - Monitor memory usage\n",
    "   - Scale up only when needed\n",
    "\n",
    "2. **Memory Efficiency**\n",
    "   - Use Hashes instead of separate keys (30-50% savings)\n",
    "   - Set TTLs on cache entries\n",
    "   - Choose appropriate data structures\n",
    "\n",
    "3. **Monitoring**\n",
    "   - Track memory usage trends\n",
    "   - Set alerts at 80% capacity\n",
    "   - Review eviction metrics\n",
    "\n",
    "4. **Cost Calculation**\n",
    "   - Every 1 GB saved = ~$50-80/month\n",
    "   - Proper TTLs can reduce costs by 30-50%\n",
    "   - Right data structures save 20-40% memory\n",
    "\n",
    "### üîß Optimization Checklist\n",
    "\n",
    "- ‚úÖ Set maxmemory and eviction policy\n",
    "- ‚úÖ Use Hashes for multi-field objects\n",
    "- ‚úÖ Set TTLs on all cache keys\n",
    "- ‚úÖ Monitor memory usage\n",
    "- ‚úÖ Regular memory analysis\n",
    "- ‚úÖ Review and optimize key patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Great Job!\n",
    "\n",
    "You now know how to optimize Redis costs and memory usage!\n"
]))

notebook = {
    "cells": cells,
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.11.0"}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

with open("workshops/deploy-redis-for-developers-amr/module-05-cost-optimization--operational-excellence/cost-optimization-lab.ipynb", "w") as f:
    json.dump(notebook, f, indent=2)

print(f"‚úÖ Module 5 notebook created ({len(cells)} cells)")
PYTHON5

echo "‚úÖ Module 5 complete"
echo

# Module 10 - Troubleshooting & Migration (Large notebook with 9 Python blocks)
python3 << 'PYTHON10'
import json

def mk_md(source):
    return {"cell_type": "markdown", "metadata": {}, "source": source}

def mk_code(source):
    return {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": source}

cells = []

cells.append(mk_md([
    "# Module 10: Troubleshooting & Migration\n",
    "\n",
    "## üéØ Interactive Lab: Diagnostics & Data Migration\n",
    "\n",
    "**Duration:** 60 minutes  \n",
    "**Level:** Advanced  \n",
    "\n",
    "In this lab, you'll:\n",
    "- üîç Use diagnostic commands to troubleshoot issues\n",
    "- üìä Analyze slow queries and performance\n",
    "- üöÄ Migrate data between Redis instances\n",
    "- üõ†Ô∏è Use RIOT for bulk operations\n",
    "- ‚úÖ Implement migration best practices\n",
    "\n",
    "---\n"
]))

cells.append(mk_md(["## Part 1: Setup\n"]))

cells.append(mk_code([
    "!pip install -q redis\n",
    "\n",
    "import redis\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    r.ping()\n",
    "    print('‚úÖ Connected to Redis')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Connection failed: {e}')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 2: Diagnostic Commands\n",
    "\n",
    "Essential commands for troubleshooting Redis:\n"
]))

cells.append(mk_code([
    "def run_diagnostics():\n",
    "    \"\"\"Run comprehensive Redis diagnostics\"\"\"\n",
    "    \n",
    "    print('üîç Redis Diagnostics Report')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Server info\n",
    "    info = r.info('server')\n",
    "    print(f'\\nüìä Server Info:')\n",
    "    print(f'   Redis version: {info[\"redis_version\"]}')\n",
    "    print(f'   Uptime: {info[\"uptime_in_days\"]} days')\n",
    "    print(f'   Process ID: {info[\"process_id\"]}')\n",
    "    \n",
    "    # Memory stats\n",
    "    mem_info = r.info('memory')\n",
    "    used_mb = mem_info['used_memory'] / 1024 / 1024\n",
    "    peak_mb = mem_info['used_memory_peak'] / 1024 / 1024\n",
    "    print(f'\\nüíæ Memory:')\n",
    "    print(f'   Used: {used_mb:.2f} MB')\n",
    "    print(f'   Peak: {peak_mb:.2f} MB')\n",
    "    print(f'   Fragmentation: {mem_info.get(\"mem_fragmentation_ratio\", 1.0):.2f}')\n",
    "    \n",
    "    # Stats\n",
    "    stats = r.info('stats')\n",
    "    print(f'\\nüìà Stats:')\n",
    "    print(f'   Total connections: {stats[\"total_connections_received\"]}')\n",
    "    print(f'   Total commands: {stats[\"total_commands_processed\"]}')\n",
    "    print(f'   Keyspace hits: {stats.get(\"keyspace_hits\", 0)}')\n",
    "    print(f'   Keyspace misses: {stats.get(\"keyspace_misses\", 0)}')\n",
    "    \n",
    "    # Calculate hit rate\n",
    "    hits = stats.get('keyspace_hits', 0)\n",
    "    misses = stats.get('keyspace_misses', 0)\n",
    "    total = hits + misses\n",
    "    if total > 0:\n",
    "        hit_rate = (hits / total) * 100\n",
    "        print(f'   Hit rate: {hit_rate:.1f}%')\n",
    "    \n",
    "    # Clients\n",
    "    clients = r.info('clients')\n",
    "    print(f'\\nüë• Clients:')\n",
    "    print(f'   Connected: {clients[\"connected_clients\"]}')\n",
    "    print(f'   Blocked: {clients.get(\"blocked_clients\", 0)}')\n",
    "    \n",
    "    # Keyspace\n",
    "    keyspace = r.info('keyspace')\n",
    "    print(f'\\nüîë Keyspace:')\n",
    "    if keyspace:\n",
    "        for db, info in keyspace.items():\n",
    "            print(f'   {db}: {info}')\n",
    "    else:\n",
    "        print('   No keys found')\n",
    "\n",
    "# Run diagnostics\n",
    "run_diagnostics()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 3: SLOWLOG Analysis\n",
    "\n",
    "Identify slow queries affecting performance:\n"
]))

cells.append(mk_code([
    "def analyze_slowlog():\n",
    "    \"\"\"Analyze slow query log\"\"\"\n",
    "    \n",
    "    print('üêå Slow Query Analysis')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Get slowlog entries\n",
    "    slowlog = r.slowlog_get(10)  # Last 10 entries\n",
    "    \n",
    "    if not slowlog:\n",
    "        print('‚úÖ No slow queries found!')\n",
    "        print('   This is good - your queries are fast')\n",
    "        return\n",
    "    \n",
    "    print(f'\\n‚ö†Ô∏è  Found {len(slowlog)} slow queries:')\n",
    "    print()\n",
    "    \n",
    "    for entry in slowlog:\n",
    "        # entry is a dict with: id, start_time, duration, command\n",
    "        duration_ms = entry['duration'] / 1000  # Convert to ms\n",
    "        command = ' '.join(str(arg) for arg in entry['command'])\n",
    "        \n",
    "        print(f'‚è±Ô∏è  Duration: {duration_ms:.2f} ms')\n",
    "        print(f'   Command: {command[:100]}')\n",
    "        print()\n",
    "\n",
    "# First, generate some activity\n",
    "print('üìù Generating sample queries...')\n",
    "for i in range(100):\n",
    "    r.set(f'test:{i}', f'value_{i}')\n",
    "    r.get(f'test:{i}')\n",
    "\n",
    "# Analyze slowlog\n",
    "analyze_slowlog()\n",
    "\n",
    "# Show slowlog configuration\n",
    "print('‚öôÔ∏è  Slowlog Configuration:')\n",
    "threshold = r.config_get('slowlog-log-slower-than')\n",
    "max_len = r.config_get('slowlog-max-len')\n",
    "print(f'   Threshold: {threshold} microseconds')\n",
    "print(f'   Max entries: {max_len}')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 4: CLIENT LIST Analysis\n",
    "\n",
    "Monitor connected clients and identify issues:\n"
]))

cells.append(mk_code([
    "def analyze_clients():\n",
    "    \"\"\"Analyze connected clients\"\"\"\n",
    "    \n",
    "    print('üë• Client Connection Analysis')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Get client list\n",
    "    clients = r.client_list()\n",
    "    \n",
    "    if not clients:\n",
    "        print('‚ÑπÔ∏è  No clients connected')\n",
    "        return\n",
    "    \n",
    "    print(f'\\nüìä Total clients: {len(clients)}')\n",
    "    print()\n",
    "    \n",
    "    # Analyze clients\n",
    "    idle_times = []\n",
    "    commands = []\n",
    "    \n",
    "    for client in clients:\n",
    "        idle = client.get('idle', 0)\n",
    "        idle_times.append(idle)\n",
    "        \n",
    "        cmd = client.get('cmd', 'unknown')\n",
    "        commands.append(cmd)\n",
    "    \n",
    "    # Show summary\n",
    "    if idle_times:\n",
    "        avg_idle = sum(idle_times) / len(idle_times)\n",
    "        max_idle = max(idle_times)\n",
    "        \n",
    "        print(f'‚è±Ô∏è  Idle Times:')\n",
    "        print(f'   Average: {avg_idle:.1f} seconds')\n",
    "        print(f'   Maximum: {max_idle} seconds')\n",
    "        \n",
    "        # Warn about idle connections\n",
    "        long_idle = [c for c in clients if c.get('idle', 0) > 300]\n",
    "        if long_idle:\n",
    "            print(f'\\n‚ö†Ô∏è  {len(long_idle)} clients idle > 5 minutes')\n",
    "            print('   Consider setting timeout for idle connections')\n",
    "    \n",
    "    # Show sample clients\n",
    "    print(f'\\nüìã Sample Clients (showing first 3):')\n",
    "    for i, client in enumerate(clients[:3]):\n",
    "        print(f'\\n   Client {i+1}:')\n",
    "        print(f'      Address: {client.get(\"addr\", \"unknown\")}')\n",
    "        print(f'      Age: {client.get(\"age\", 0)} seconds')\n",
    "        print(f'      Idle: {client.get(\"idle\", 0)} seconds')\n",
    "        print(f'      Last command: {client.get(\"cmd\", \"unknown\")}')\n",
    "\n",
    "# Run analysis\n",
    "analyze_clients()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 5: Memory Analysis\n",
    "\n",
    "Deep dive into memory usage patterns:\n"
]))

cells.append(mk_code([
    "def analyze_memory_by_pattern():\n",
    "    \"\"\"Analyze memory usage by key patterns\"\"\"\n",
    "    \n",
    "    print('üíæ Memory Usage by Key Pattern')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Create sample data with different patterns\n",
    "    patterns = {\n",
    "        'cache:': 50,\n",
    "        'session:': 30,\n",
    "        'user:': 20,\n",
    "    }\n",
    "    \n",
    "    for pattern, count in patterns.items():\n",
    "        for i in range(count):\n",
    "            r.set(f'{pattern}{i}', 'x' * 100)\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print('\\nüìä Key Distribution:')\n",
    "    print()\n",
    "    \n",
    "    total_keys = 0\n",
    "    for pattern in patterns.keys():\n",
    "        keys = r.keys(f'{pattern}*')\n",
    "        count = len(keys)\n",
    "        total_keys += count\n",
    "        \n",
    "        # Sample memory usage\n",
    "        if keys:\n",
    "            sample_size = r.memory_usage(keys[0]) if hasattr(r, 'memory_usage') else 100\n",
    "            estimated_mb = (count * sample_size) / 1024 / 1024\n",
    "            \n",
    "            print(f'   {pattern:<12} {count:>5} keys  ~{estimated_mb:.2f} MB')\n",
    "    \n",
    "    print(f'\\n   Total: {total_keys} keys')\n",
    "    \n",
    "    # Get overall memory\n",
    "    info = r.info('memory')\n",
    "    used_mb = info['used_memory'] / 1024 / 1024\n",
    "    print(f'\\nüíæ Total memory used: {used_mb:.2f} MB')\n",
    "\n",
    "# Run analysis\n",
    "analyze_memory_by_pattern()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 6: Data Migration Preparation\n",
    "\n",
    "Prepare for migrating data between Redis instances:\n"
]))

cells.append(mk_code([
    "def prepare_migration_report():\n",
    "    \"\"\"Generate pre-migration report\"\"\"\n",
    "    \n",
    "    print('üìã Pre-Migration Checklist')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Count keys\n",
    "    total_keys = r.dbsize()\n",
    "    print(f'\\nüìä Data Inventory:')\n",
    "    print(f'   Total keys: {total_keys:,}')\n",
    "    \n",
    "    # Sample key types\n",
    "    if total_keys > 0:\n",
    "        keys_sample = r.keys('*')[:100]  # Sample first 100\n",
    "        types = {}\n",
    "        \n",
    "        for key in keys_sample:\n",
    "            key_type = r.type(key)\n",
    "            types[key_type] = types.get(key_type, 0) + 1\n",
    "        \n",
    "        print(f'\\nüîë Key Types (sample of {len(keys_sample)}):')\n",
    "        for key_type, count in types.items():\n",
    "            print(f'   {key_type:<10} {count:>5} keys')\n",
    "    \n",
    "    # Memory info\n",
    "    mem_info = r.info('memory')\n",
    "    used_mb = mem_info['used_memory'] / 1024 / 1024\n",
    "    peak_mb = mem_info['used_memory_peak'] / 1024 / 1024\n",
    "    \n",
    "    print(f'\\nüíæ Memory Requirements:')\n",
    "    print(f'   Current usage: {used_mb:.2f} MB')\n",
    "    print(f'   Peak usage: {peak_mb:.2f} MB')\n",
    "    print(f'   Recommended target: {peak_mb * 1.2:.2f} MB (with 20% buffer)')\n",
    "    \n",
    "    # Estimate migration time\n",
    "    if total_keys > 0:\n",
    "        # Rough estimate: 1000 keys/second\n",
    "        estimated_seconds = total_keys / 1000\n",
    "        estimated_minutes = estimated_seconds / 60\n",
    "        \n",
    "        print(f'\\n‚è±Ô∏è  Migration Estimate:')\n",
    "        print(f'   Estimated time: {estimated_minutes:.1f} minutes')\n",
    "        print(f'   (at ~1000 keys/second)')\n",
    "    \n",
    "    # Checklist\n",
    "    print(f'\\n‚úÖ Pre-Migration Checklist:')\n",
    "    checklist = [\n",
    "        'Backup source Redis instance',\n",
    "        'Provision target instance with sufficient memory',\n",
    "        'Test connectivity to target instance',\n",
    "        'Plan maintenance window',\n",
    "        'Prepare rollback plan',\n",
    "        'Notify stakeholders',\n",
    "    ]\n",
    "    \n",
    "    for item in checklist:\n",
    "        print(f'   ‚ñ° {item}')\n",
    "\n",
    "# Generate report\n",
    "prepare_migration_report()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 7: Simple Key Migration\n",
    "\n",
    "Migrate keys between Redis instances (demo with same instance):\n"
]))

cells.append(mk_code([
    "def migrate_keys_demo():\n",
    "    \"\"\"Demonstrate key migration pattern\"\"\"\n",
    "    \n",
    "    print('üöÄ Key Migration Demo')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Create sample data in \"source\" namespace\n",
    "    print('\\nüìù Creating source data...')\n",
    "    source_prefix = 'source:'\n",
    "    target_prefix = 'target:'\n",
    "    \n",
    "    # Clear any existing data\n",
    "    for key in r.keys(f'{source_prefix}*') + r.keys(f'{target_prefix}*'):\n",
    "        r.delete(key)\n",
    "    \n",
    "    # Create source data\n",
    "    for i in range(10):\n",
    "        r.set(f'{source_prefix}key:{i}', f'value_{i}')\n",
    "        r.setex(f'{source_prefix}temp:{i}', 3600, f'temp_{i}')  # With TTL\n",
    "    \n",
    "    source_keys = r.keys(f'{source_prefix}*')\n",
    "    print(f'   Created {len(source_keys)} source keys')\n",
    "    \n",
    "    # Migration function\n",
    "    def migrate_key(source_key, target_key):\n",
    "        \"\"\"Migrate a single key with its TTL\"\"\"\n",
    "        # Get value\n",
    "        value = r.get(source_key)\n",
    "        if value is None:\n",
    "            return False\n",
    "        \n",
    "        # Get TTL\n",
    "        ttl = r.ttl(source_key)\n",
    "        \n",
    "        # Set in target\n",
    "        if ttl > 0:\n",
    "            r.setex(target_key, ttl, value)\n",
    "        else:\n",
    "            r.set(target_key, value)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # Migrate keys\n",
    "    print(f'\\nüîÑ Migrating keys...')\n",
    "    migrated = 0\n",
    "    \n",
    "    for source_key in source_keys:\n",
    "        # Convert source: prefix to target: prefix\n",
    "        target_key = source_key.replace(source_prefix, target_prefix)\n",
    "        \n",
    "        if migrate_key(source_key, target_key):\n",
    "            migrated += 1\n",
    "    \n",
    "    print(f'   Migrated {migrated} keys')\n",
    "    \n",
    "    # Verify migration\n",
    "    target_keys = r.keys(f'{target_prefix}*')\n",
    "    print(f'\\n‚úÖ Verification:')\n",
    "    print(f'   Source keys: {len(source_keys)}')\n",
    "    print(f'   Target keys: {len(target_keys)}')\n",
    "    print(f'   Migration complete: {len(source_keys) == len(target_keys)}')\n",
    "    \n",
    "    # Show sample\n",
    "    if target_keys:\n",
    "        sample_key = target_keys[0]\n",
    "        sample_value = r.get(sample_key)\n",
    "        sample_ttl = r.ttl(sample_key)\n",
    "        \n",
    "        print(f'\\nüìã Sample migrated key:')\n",
    "        print(f'   Key: {sample_key}')\n",
    "        print(f'   Value: {sample_value}')\n",
    "        print(f'   TTL: {sample_ttl} seconds' if sample_ttl > 0 else '   TTL: No expiration')\n",
    "\n",
    "# Run migration demo\n",
    "migrate_keys_demo()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 8: Performance Testing\n",
    "\n",
    "Test performance before and after migration:\n"
]))

cells.append(mk_code([
    "def benchmark_performance():\n",
    "    \"\"\"Benchmark Redis performance\"\"\"\n",
    "    \n",
    "    print('‚ö° Performance Benchmark')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    import statistics\n",
    "    \n",
    "    operations = 1000\n",
    "    \n",
    "    # Test SET performance\n",
    "    print(f'\\nüìù Testing SET ({operations} operations)...')\n",
    "    set_times = []\n",
    "    \n",
    "    for i in range(operations):\n",
    "        start = time.perf_counter()\n",
    "        r.set(f'perf:test:{i}', f'value_{i}')\n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        set_times.append(elapsed)\n",
    "    \n",
    "    print(f'   Average: {statistics.mean(set_times):.2f} ms')\n",
    "    print(f'   Median: {statistics.median(set_times):.2f} ms')\n",
    "    print(f'   P95: {sorted(set_times)[int(len(set_times) * 0.95)]:.2f} ms')\n",
    "    print(f'   P99: {sorted(set_times)[int(len(set_times) * 0.99)]:.2f} ms')\n",
    "    \n",
    "    # Test GET performance\n",
    "    print(f'\\nüìñ Testing GET ({operations} operations)...')\n",
    "    get_times = []\n",
    "    \n",
    "    for i in range(operations):\n",
    "        start = time.perf_counter()\n",
    "        r.get(f'perf:test:{i}')\n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        get_times.append(elapsed)\n",
    "    \n",
    "    print(f'   Average: {statistics.mean(get_times):.2f} ms')\n",
    "    print(f'   Median: {statistics.median(get_times):.2f} ms')\n",
    "    print(f'   P95: {sorted(get_times)[int(len(get_times) * 0.95)]:.2f} ms')\n",
    "    print(f'   P99: {sorted(get_times)[int(len(get_times) * 0.99)]:.2f} ms')\n",
    "    \n",
    "    # Cleanup\n",
    "    for i in range(operations):\n",
    "        r.delete(f'perf:test:{i}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Benchmark complete')\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_performance()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Part 9: Troubleshooting Common Issues\n",
    "\n",
    "### Common Problems and Solutions\n"
]))

cells.append(mk_code([
    "def troubleshooting_guide():\n",
    "    \"\"\"Display troubleshooting guide\"\"\"\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            'problem': 'High Memory Usage',\n",
    "            'symptoms': ['Memory close to limit', 'Evictions occurring'],\n",
    "            'solutions': [\n",
    "                'Check for keys without TTL',\n",
    "                'Analyze key patterns with MEMORY USAGE',\n",
    "                'Consider using Hashes instead of separate keys',\n",
    "                'Review maxmemory-policy setting',\n",
    "                'Scale up to larger instance'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Slow Query Performance',\n",
    "            'symptoms': ['High latency', 'Slow response times'],\n",
    "            'solutions': [\n",
    "                'Check SLOWLOG for expensive commands',\n",
    "                'Avoid KEYS command in production',\n",
    "                'Use pipelining for multiple operations',\n",
    "                'Check network latency',\n",
    "                'Consider using connection pooling'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Connection Issues',\n",
    "            'symptoms': ['Timeout errors', 'Connection refused'],\n",
    "            'solutions': [\n",
    "                'Verify network connectivity',\n",
    "                'Check firewall rules',\n",
    "                'Verify authentication credentials',\n",
    "                'Check maxclients setting',\n",
    "                'Review timeout configuration'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Memory Fragmentation',\n",
    "            'symptoms': ['High RSS memory', 'Fragmentation ratio > 1.5'],\n",
    "            'solutions': [\n",
    "                'Monitor fragmentation ratio',\n",
    "                'Consider enabling active defragmentation',\n",
    "                'Restart Redis during maintenance window',\n",
    "                'Review key patterns and sizes'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print('üîß Troubleshooting Guide')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f'\\n{i}. {issue[\"problem\"]}')\n",
    "        print('   ' + '-' * 40)\n",
    "        \n",
    "        print('   Symptoms:')\n",
    "        for symptom in issue['symptoms']:\n",
    "            print(f'     ‚Ä¢ {symptom}')\n",
    "        \n",
    "        print('   Solutions:')\n",
    "        for solution in issue['solutions']:\n",
    "            print(f'     ‚úì {solution}')\n",
    "\n",
    "# Display guide\n",
    "troubleshooting_guide()\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## Cleanup\n"
]))

cells.append(mk_code([
    "# Clean up all test data\n",
    "patterns = ['test:*', 'cache:*', 'session:*', 'user:*', 'source:*', 'target:*', 'perf:*']\n",
    "total_deleted = 0\n",
    "\n",
    "for pattern in patterns:\n",
    "    keys = r.keys(pattern)\n",
    "    if keys:\n",
    "        deleted = r.delete(*keys)\n",
    "        total_deleted += deleted\n",
    "\n",
    "print(f'‚úÖ Cleanup complete: {total_deleted} keys deleted')\n"
]))

cells.append(mk_md([
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### üîç Diagnostic Tools\n",
    "\n",
    "1. **INFO Command**\n",
    "   - Server, memory, stats, clients, keyspace\n",
    "   - Run regularly for monitoring\n",
    "   - Track trends over time\n",
    "\n",
    "2. **SLOWLOG**\n",
    "   - Identify expensive queries\n",
    "   - Set appropriate threshold\n",
    "   - Optimize slow commands\n",
    "\n",
    "3. **CLIENT LIST**\n",
    "   - Monitor connections\n",
    "   - Identify idle clients\n",
    "   - Track client activity\n",
    "\n",
    "4. **MEMORY USAGE**\n",
    "   - Analyze key memory\n",
    "   - Find memory hogs\n",
    "   - Optimize data structures\n",
    "\n",
    "### üöÄ Migration Best Practices\n",
    "\n",
    "1. **Planning**\n",
    "   - Inventory source data\n",
    "   - Estimate migration time\n",
    "   - Plan maintenance window\n",
    "\n",
    "2. **Execution**\n",
    "   - Backup before migration\n",
    "   - Test with sample data\n",
    "   - Monitor progress\n",
    "   - Verify data integrity\n",
    "\n",
    "3. **Tools**\n",
    "   - DUMP/RESTORE for single keys\n",
    "   - RIOT for bulk migration\n",
    "   - Replication for live migration\n",
    "   - Custom scripts for complex scenarios\n",
    "\n",
    "4. **Validation**\n",
    "   - Compare key counts\n",
    "   - Verify sample data\n",
    "   - Test performance\n",
    "   - Check TTLs preserved\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting Checklist\n",
    "\n",
    "- ‚úÖ Run INFO to get overview\n",
    "- ‚úÖ Check SLOWLOG for slow queries\n",
    "- ‚úÖ Monitor memory usage\n",
    "- ‚úÖ Review client connections\n",
    "- ‚úÖ Check fragmentation ratio\n",
    "- ‚úÖ Verify network connectivity\n",
    "- ‚úÖ Review configuration settings\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Outstanding Work!\n",
    "\n",
    "You now have the skills to troubleshoot and migrate Redis successfully!\n"
]))

notebook = {
    "cells": cells,
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.11.0"}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

with open("workshops/deploy-redis-for-developers-amr/module-10-troubleshooting--migration/troubleshooting-migration-lab.ipynb", "w") as f:
    json.dump(notebook, f, indent=2)

print(f"‚úÖ Module 10 notebook created ({len(cells)} cells)")
PYTHON10

echo "‚úÖ Module 10 complete"
echo

echo "üéâ All remaining notebooks generated!"
echo "  - Module 5: Cost Optimization (15 cells)"
echo "  - Module 10: Troubleshooting & Migration (16 cells)"
